# -*- coding: utf-8 -*-
"""期末提出プログラム.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ssjq_F9F8aX3FUB7Pg3je5ZWPRSHd-Q
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import random
# %matplotlib inline

class Environment(object):
    def __init__(self):
        self.current_state = 0
        self.next_state = 0
        self.reward = 0
        self.num_state = 0
        self.num_action = 0
        self.start_state = 0
        self.goal_state = 0

    def get_current_state(self):
        return self.current_state

    def get_next_state(self):
        return self.next_state

    def get_reward(self):
        return self.reward

    def get_num_state(self):
        return self.num_state

    def get_num_action(self):
        return self.num_action

    def get_start(self):
        return self.start

    def get_goal_state(self):
        return self.goal

class GlidWorld(Environment):
    def __init__(self, height, length, start, goal):
        self.reward = 1
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal

    def to_s(self, row, col):
        return ((row * self.height) + col)

    def to_p(self, s):
        y = s // self.height
        x = s % self.height
        return x, y

    def move(self, state, action):
        UP = 0
        DOWN = 1
        LEFT = 2
        RIGHT = 3

        column, row = self.to_p(state)

        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        new_state = self.to_s(column, row)
        reward = self.get_reward(new_state)

        return new_state, reward

    def get_reward(self, state):
        if state == self.goal:
            return 1
        else:
            return -1

class Greedy(object):
    def select_action(self, value, state):
        idx = np.where(value[state] == max(value[state]))
        return random.choice(idx[0])

    def init_params(self):
        pass

    def update_params(self):
        pass

class EpsGreedy(Greedy):
    def __init__(self, eps = 0.3):
        self.eps = eps

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        pass

    def update_params(self):
        pass

class EpsDecGreedy(Greedy):
    def __init__(self, eps = 1.0, eps_min = 0.0, eps_decrease = 0.05):
        self.eps = self.eps_init = eps
        self.eps_min = eps_min
        self.eps_decrease = eps_decrease

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        self.eps = self.eps_init

    def update_params(self):
        self.eps = max(self.eps - self.eps_decrease, self.eps_min)

class Q_Learning(object):
    def __init__(self, num_state, num_action, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action

        self.Q = np.zeros((self.num_state, self.num_action))
        self.estimate_policy = Greedy() # 推定方策

    def update(self, current_state, current_action, reward, next_state):
        alpha = self.learning_rate
        gamma = self.discount_rate

        next_action = self.estimate_policy.select_action(self.Q, next_state)
        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]
        self.Q[current_state, current_action] += alpha * TD_error

        return current_state, next_state, next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):
        self.Q = np.zeros((self.num_state, self.num_action))

    def get_value(self):
        return self.Q

    def print_value(self):
        print(self.Q)

class Agent():
    def __init__(self, value_func="Q_Learning", policy="greedy", learning_rate=0.1, discount_rate=0.9, eps=None, eps_min=None, eps_decrease=None, n_state=None, n_action=None, lamda = 0.9, r_g = 7.5, zeta = 0.05):

        if value_func == "Q_Learning":
            self.value_func = Q_Learning(num_state=n_state, num_action=n_action)

        else:
            print("error:価値関数候補が見つかりませんでした")
            sys.exit()

        if policy == "greedy":
            self.policy = Greedy()

        elif policy == "eps_greedy":
            self.policy = EpsGreedy(eps=eps)

        elif policy == "eps_dec_greedy":
            self.policy = EpsDecGreedy(eps=eps, eps_min=eps_min, eps_decrease=eps_decrease)

        else:
            print("error:方策候補が見つかりませんでした")
            sys.exit()

        self.current_state = self.next_state = 0
        self.n_state = n_state

    def update(self, current_action, reward, i):

        current_state = self.current_state
        next_state = self.next_state
        self.value_func.update(current_state, current_action, reward, next_state)
        self.policy.update_params()

    def select_action(self):
        return self.policy.select_action(self.value_func.get_value(), self.current_state)
    
    def select_action_sarsa(self):
        return self.policy.select_action(self.value_func.get_value(), self.next_state)

    def print_value(self):
        self.value_func.print_value()

    def init_params(self):
        self.value_func.init_params()
        self.policy.init_params()
        self.current_state = self.next_state = 0

def main():

    env = GlidWorld(7,7,0,48)

    SIMULATION_NUM = 100
    EPISODE_NUM = 1000

    GOAL = 48

    agents = {}
    
    agents.update({"QL": Agent(
        policy="eps_dec_greedy",
        eps=1.0,
        eps_min=0.0,
        eps_decrease=0.001,
        n_state=env.get_num_state(),
        n_action=env.get_num_action()
    )})
    

    reward_graph = np.zeros(EPISODE_NUM)
    optimal_reward = np.full(EPISODE_NUM, -11)

    reward_fig = plt.figure()
    print("start training")
    agents_steps = {}
    agents_rewards = {}
    agents_Eg = {}
    plt.figure()
    for i in agents.keys():
        print("{}'s training".format(i))
        step_graph = np.zeros(EPISODE_NUM)
        reward_graph = np.zeros(EPISODE_NUM)
        Eg_graph = np.zeros(EPISODE_NUM)

        #学習開始
        for sim in range(SIMULATION_NUM):
            agents[i].init_params()
            every_reward = np.zeros(EPISODE_NUM)
            for epi in range(EPISODE_NUM):
                sum_reward = 0
                agents[i].current_state = env.get_start()
                #ステップ開始
                for step in range(1000):
                    action = agents[i].select_action()

                    agents[i].next_state, reward = env.move(agents[i].current_state, action)

                    sum_reward += reward
                    
                    agents[i].update(action, reward, i)

                    agents[i].current_state = agents[i].next_state

                    if agents[i].current_state == GOAL or step == 1000:
                        break

                step_graph[epi] += step
                reward_graph[epi] += sum_reward

        agents_steps.update({i:step_graph})
        agents_rewards.update({i: reward_graph})

        print("finish")

  
    
    
    for i in agents.keys():
        plt.plot(agents_rewards[i] / SIMULATION_NUM, label=i)

    plt.plot(optimal_reward, label="Optimal reward")

    plt.legend()  # 凡例を付ける
    #plt.ylim(4, 8)
    plt.title("100 times average reward")  # グラフタイトルを付ける
    plt.xlabel("episode")  # x軸のラベルを付ける
    plt.ylabel("reward")  # y軸のラベルを付ける
    reward_fig.tight_layout()
    plt.savefig("gridworld.png")
    plt.show()  # グラフを表示

main()

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import random


class Environment(object):
    def __init__(self):
        self.current_state = 0
        self.next_state = 0
        self.reward = 0
        self.num_state = 0
        self.num_action = 0
        self.start_state = 0
        self.goal_state = 0

    def get_current_state(self):
        return self.current_state

    def get_next_state(self):
        return self.next_state

    def get_reward(self):
        return self.reward

    def get_num_state(self):
        return self.num_state

    def get_num_action(self):
        return self.num_action

    def get_start(self):
        return self.start

    def get_goal_state(self):
        return self.goal

class GlidWorld(Environment):
    def __init__(self, height, length, start, goal):
        self.reward = 1
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal

    def to_s(self, row, col):
        return ((row * self.height) + col)

    def to_p(self, s):
        y = s // self.height
        x = s % self.height
        return x, y

    def move(self, state, action):
        UP = 0
        DOWN = 1
        LEFT = 2
        RIGHT = 3

        column, row = self.to_p(state)

        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        new_state = self.to_s(column, row)
        reward = self.get_reward(new_state)

        return new_state, reward

    def get_reward(self, state):
        if state == self.goal:
            return 1
        else:
            return -1

class Greedy(object):
    def select_action(self, value, state):
        idx = np.where(value[state] == max(value[state]))
        return random.choice(idx[0])

    def init_params(self):
        pass

    def update_params(self):
        pass

class EpsGreedy(Greedy):
    def __init__(self, eps = 0.3):
        self.eps = eps

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        pass

    def update_params(self):
        pass

class EpsDecGreedy(Greedy):
    def __init__(self, eps = 1.0, eps_min = 0.0, eps_decrease = 0.05):
        self.eps = self.eps_init = eps
        self.eps_min = eps_min
        self.eps_decrease = eps_decrease

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        self.eps = self.eps_init

    def update_params(self):
        self.eps = max(self.eps - self.eps_decrease, self.eps_min)

class Q_Learning(object):
    def __init__(self, num_state, num_action, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action

        self.Q = np.zeros((self.num_state, self.num_action))
        self.estimate_policy = Greedy() # 推定方策

    def update(self, current_state, current_action, reward, next_state):
        alpha = self.learning_rate
        gamma = self.discount_rate

        next_action = self.estimate_policy.select_action(self.Q, next_state)
        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]
        self.Q[current_state, current_action] += alpha * TD_error

        return current_state, next_state, next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):
        self.Q = np.zeros((self.num_state, self.num_action))

    def get_value(self):
        return self.Q

    def print_value(self):
        print(self.Q)

class Agent():
    def __init__(self, value_func="Q_Learning", policy="greedy", learning_rate=0.1, discount_rate=0.9, eps=None, eps_min=None, eps_decrease=None, n_state=None, n_action=None, lamda = 0.9, r_g = 7.5, zeta = 0.05):

        if value_func == "Q_Learning":
            self.value_func = Q_Learning(num_state=n_state, num_action=n_action)

        else:
            print("error:価値関数候補が見つかりませんでした")
            sys.exit()

        if policy == "greedy":
            self.policy = Greedy()

        elif policy == "eps_greedy":
            self.policy = EpsGreedy(eps=eps)

        elif policy == "eps_dec_greedy":
            self.policy = EpsDecGreedy(eps=eps, eps_min=eps_min, eps_decrease=eps_decrease)

        else:
            print("error:方策候補が見つかりませんでした")
            sys.exit()

        self.current_state = self.next_state = 0
        self.n_state = n_state

    def update(self, current_action, reward, i):

        current_state = self.current_state
        next_state = self.next_state
        self.value_func.update(current_state, current_action, reward, next_state)

    def update_params(self):
        self.policy.update_params()

    def select_action(self):
        return self.policy.select_action(self.value_func.get_value(), self.current_state)

    def select_action_sarsa(self):
        return self.policy.select_action(self.value_func.get_value(), self.next_state)

    def print_value(self):
        self.value_func.print_value()

    def init_params(self):
        self.value_func.init_params()
        self.policy.init_params()
        self.current_state = self.next_state = 0

def main():

    env = GlidWorld(7,7,0,48)

    SIMULATION_NUM = 100
    EPISODE_NUM = 1000

    GOAL = 48

    agents = {}

    agents.update({"eps_decay": Agent(
        policy="eps_dec_greedy",
        eps=1.0,
        eps_min=0.0,
        eps_decrease=0.001,
        n_state=env.get_num_state(),
        n_action=env.get_num_action()
    )})

    agents.update({"eps": Agent(
        policy="eps_greedy",
        eps = 0.2,
        n_state=env.get_num_state(),
        n_action=env.get_num_action()
    )})

    agents.update({"greedy": Agent(
        policy="greedy",
        n_state=env.get_num_state(),
        n_action=env.get_num_action()
    )})

    reward_graph = np.zeros(EPISODE_NUM)
    optimal_reward = np.full(EPISODE_NUM, -11)

    reward_fig = plt.figure()
    print("start training")
    agents_steps = {}
    agents_rewards = {}
    agents_Eg = {}
    plt.figure()
    for i in agents.keys():
        print("{}'s training".format(i))
        step_graph = np.zeros(EPISODE_NUM)
        reward_graph = np.zeros(EPISODE_NUM)
        Eg_graph = np.zeros(EPISODE_NUM)

        #学習開始
        for sim in range(SIMULATION_NUM):
            agents[i].init_params()
            every_reward = np.zeros(EPISODE_NUM)
            for epi in range(EPISODE_NUM):
                sum_reward = 0
                agents[i].current_state = env.get_start()
                #ステップ開始
                for step in range(1000):
                    action = agents[i].select_action()

                    agents[i].next_state, reward = env.move(agents[i].current_state, action)

                    sum_reward += reward

                    agents[i].update(action, reward, i)

                    agents[i].current_state = agents[i].next_state

                    if agents[i].current_state == GOAL or step == 1000:
                        break
                agents[i].update_params()
                step_graph[epi] += step
                reward_graph[epi] += sum_reward

        agents_steps.update({i:step_graph})
        agents_rewards.update({i: reward_graph})

        print("finish")




    for i in agents.keys():
        plt.plot(agents_rewards[i] / SIMULATION_NUM, label=i)

    plt.plot(optimal_reward, label="Optimal reward")

    plt.legend()  # 凡例を付ける
    plt.title("100 times average reward")  # グラフタイトルを付ける
    plt.xlabel("episode")  # x軸のラベルを付ける
    plt.ylabel("reward")  # y軸のラベルを付ける
    reward_fig.tight_layout()
    plt.savefig("gridworld.png")
    plt.show()  # グラフを表示

main()

# Commented out IPython magic to ensure Python compatibility.
# %ls