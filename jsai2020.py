# -*- coding: utf-8 -*-
"""若林＿JSAI2020.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mFpYCF69gUXz6balPyYW3Q9FG6Tq7unk

# 一番上
"""

import numpy as np
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
from matplotlib import animation, rc
from matplotlib.animation import PillowWriter
from IPython.display import HTML

import seaborn as sns
import random
import sys
import csv
# import gspread
# import pandas as pd
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

# auth.authenticate_user()
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# worksheet = gc.open('Environment').sheet1
# rows = worksheet.get_all_values()
# print(rows)
# # pd.DataFrame.from_records(rows)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

sns.set()
sns.set_context("notebook", font_scale = 1)

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/'My Drive'/大学/研究室/JSAI2020/img

"""# 環境"""

class Environment(object):
    def __init__(self):
        self.current_state = 0
        self.next_state = 0
        self.reward = 0
        self.num_state = 0
        self.num_action = 0
        self.start_state = 0
        self.goal_state = 0

    def get_current_state(self):
        return self.current_state

    def get_next_state(self):
        return self.next_state

    def get_reward(self):
        return self.reward

    def get_num_state(self):
        return self.num_state

    def get_num_action(self):
        return self.num_action

    def get_start(self):
        return self.start

    def get_goal_state(self):
        return self.goal

"""# グリッドワールド"""

class GlidWorld(Environment):
    def __init__(self, height, length, start, goal):
        self.reward = 1
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal

    def to_s(self, row, col):
            return ((col * self.height) + row)

    def to_p(self, s):
        y = s // self.height
        x = s % self.height
        return x, y

    def move(self, state, action):
        UP = 0
        DOWN = 2
        LEFT = 3
        RIGHT = 1

        column, row = self.to_p(state)

        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        new_state = self.to_s(column, row)
        reward = self.get_reward(new_state)

        return new_state, reward

    def get_reward(self, state):

        if state == self.goal:
            return 1

        else:
            return -1

"""# 崖ありグリッドワールド"""

class Cliff_GlidWorld(GlidWorld):
    def __init__(self, height, length, start, goal, cliff):
        self.reward = 1
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal
        self.cliff = cliff

    def to_s(self, row, col):
        return ((row * self.height) + col)

    def to_p(self, s):
        y = s // self.height
        x = s % self.height
        return x, y

    def move(self, state, action):
        UP = 0
        DOWN = 1
        LEFT = 2
        RIGHT = 3

        column, row = self.to_p(state)

        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        new_state = self.to_s(column, row)
        reward = self.get_reward(new_state)

        return new_state, reward

    def get_reward(self, state):

        if state == self.goal:
            return 1
        elif state in self.cliff:
            return -1
        else:
            return 0

"""# suboptima"""

class Cliff_optima_GlidWorld(GlidWorld):
    def __init__(self, height, length, start, goal, cliff):

        # self.reward = [1,8,2,7,4,5,3,6]
        self.reward = [8,1,5,4,7,2,6,3]
        # self.reward = [8,0,0,0,0,0,0,0]
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal
        self.cliff = cliff


    def to_s(self, row, col):
        return ((col * self.height) + row)

    def to_p(self, s):
        y = s // self.height
        x = s % self.height
        return x, y

    def move(self, state, action):
        UP = 0
        DOWN = 2
        LEFT = 3
        RIGHT = 1

        column, row = self.to_p(state)

        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        new_state = self.to_s(column, row)
        reward = self.get_reward(new_state)

        return new_state, reward

    def get_reward(self, state):

        if state in self.goal:
            return self.reward[self.goal.index(state)]
        #elif state in self.cliff:
        #    return -1
        else:
            return 0

"""# アルゴリズム

# greedy
"""

class TestGreedy(object):
    def select_action(value, state):
        idx = np.where(value[state] == max(value[state]))
        return random.choice(idx[0])

class Greedy(object):
    def select_action(self, value, state):
        idx = np.where(value[state] == max(value[state]))
        return random.choice(idx[0])

    def init_params(self):
        pass

    def update_params(self):
        pass

"""# ε- greedy"""

class EpsGreedy(Greedy):
    def __init__(self, eps = 0.3):
        self.eps = eps

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        pass

    def update_params(self):
        pass

"""#  dec ε- greedy"""

class EpsDecGreedy(Greedy):
    def __init__(self, eps = 1.0, eps_min = 0.0, eps_decrease = 0.05):
        self.eps = self.eps_init = eps
        self.eps_min = eps_min
        self.eps_decrease = eps_decrease

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        self.eps = self.eps_init

    def update_params(self):
        self.eps = max(self.eps - self.eps_decrease, self.eps_min)

"""# Q-Learning"""

class Q_Learning(object):
    def __init__(self, num_state, num_action, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action

        self.Q = np.zeros((self.num_state, self.num_action))
        self.estimate_policy = Greedy() # 推定方策

    def update(self, current_state, current_action, reward, next_state):
        alpha = self.learning_rate
        gamma = self.discount_rate

        next_action = self.estimate_policy.select_action(self.Q, next_state)
        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]
        self.Q[current_state, current_action] += alpha * TD_error

        return current_state, next_state, next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):
        self.Q = np.zeros((self.num_state, self.num_action))

    def get_value(self):
        return self.Q

    def print_value(self):
        print(self.Q)

"""# Sarsa"""

class Sarsa(object):
    def __init__(self, num_state, num_action, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action
        self.estimate_policy = Greedy() # 推定方策

        self.Q = np.zeros((self.num_state, self.num_action))

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha = self.learning_rate
        gamma = self.discount_rate

        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]
        self.Q[current_state, current_action] += alpha * TD_error

        return current_state, next_state, next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):
        self.Q = np.zeros((self.num_state, self.num_action))

    def get_value(self):
        return self.Q

    def print_value(self):
        print(self.Q)

"""# Sarsa Lambda"""

class Sarsa_lambda(Q_Learning):
    def __init__(self, num_state, num_action, lamda = 0.9, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action
        self.lamda = lamda
        self.estimate_policy = Greedy() # 推定方策
        self.eligibility = np.zeros((self.num_state, self.num_action))

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha = self.learning_rate
        gamma = self.discount_rate

        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]

        self.eligibility[current_state, current_action] += 1

        self.Q += alpha * TD_error * self.eligibility
        self.eligibility = gamma * 0.9 * self.eligibility


        return current_state, next_state, next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):

        self.Q = np.zeros((self.num_state, self.num_action))
        self.eligibility = np.zeros((self.num_state, self.num_action))

    def print_value(self):
        print(self.eligibility)

"""# Watkin Lambda"""

class Watkin_lambda(Q_Learning):
    def __init__(self, num_state, num_action, lamda = 0.9, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action
        self.lamda = lamda
        self.itit_lamda = lamda

        self.Q = np.zeros((self.num_state, self.num_action))
        self.estimate_policy = Greedy()# 推定方策
        self.eligibility = np.zeros((self.num_state, self.num_action))

    def update(self, current_state, current_action, reward, next_state, true_next_action):
        alpha = self.learning_rate
        gamma = self.discount_rate

        next_action = self.estimate_policy.select_action(self.Q, next_state)

        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]

        self.eligibility[current_state, current_action] += 1
        self.Q += alpha * TD_error * self.eligibility

        if next_action == true_next_action:
            self.eligibility= gamma * 0.4 * self.eligibility
        else:
            self.eligibility = np.zeros((self.num_state, self.num_action))

        return current_state, next_state, true_next_action

    def GRC_update(self, e_tmp):
        pass

    def init_params(self):

        self.Q = np.zeros((self.num_state, self.num_action))
        self.eligibility = np.zeros((self.num_state, self.num_action))
        self.lamda = self.itit_lamda

    def print_value(self):
        print(self.eligibility)

"""# RS + GRC(off policy)"""

class RS_value_function(Q_Learning):
    def __init__(self, num_state, num_action, learning_rate=0.1, discount_rate=0.9, r_g=7.5, zeta=0.05):

        super().__init__(num_state, num_action, learning_rate, discount_rate)

        self.r = []
        self.t_curr = []
        self.t_post = []
        self.rs = []
        self.zeta = zeta
        self.r_g = r_g
        self.gamma_g = 0.9
        self.e_g = 0
        self.n_g = 0
        self.sum_Eg = []

    def update(self, current_state, current_action, reward, next_state):
        alpha_tau = self.learning_rate
        gamma_tau = self.discount_rate

        # Q 値の更新を行う
        q_current_state, q_next_state, next_action = super().update(current_state, current_action, reward, next_state)

        # tau 値の更新
        t = self.t_curr + self.t_post
        self.t_curr[q_current_state, current_action] += 1
        self.t_post[q_current_state, current_action] += alpha_tau * (gamma_tau * t[q_next_state, next_action] - self.t_post[q_current_state, current_action])
        t = self.t_curr + self.t_post

        # 基準値 R の更新
        delta_g = min(self.e_g - self.r_g, 0)
        a = self.estimate_policy.select_action(self.Q, q_current_state)
        self.r[q_current_state] = self.Q[q_current_state][a] - self.zeta * delta_g

        # RS 値の更新
        self.rs = t * (self.Q.T - self.r).T

    def GRC_update(self, e_tmp):
        self.e_g = (e_tmp + self.gamma_g * (self.n_g * self.e_g)) / float(1 + self.gamma_g * self.n_g)
        # self.e_g = e_tmp
        self.sum_Eg.append(self.e_g)
        
        self.n_g = 1.0 + self.gamma_g * self.n_g

    def init_params(self):
        super().init_params()

        self.t_curr = np.zeros([self.num_state, self.num_action])
        self.t_post = np.zeros([self.num_state, self.num_action])
        self.rs = np.zeros([self.num_state, self.num_action])
        self.r = np.zeros(self.num_state)
        self.e_g = 0

    def get_value(self):
        return self.rs

"""# RS + GRC(on policy)"""

class RS_value_function_onpolicy(Sarsa):
    def __init__(self, num_state, num_action, learning_rate=0.1, discount_rate=0.9, r_g=7.5, zeta=0.05):

        super().__init__(num_state, num_action, learning_rate, discount_rate)

        self.r = []
        self.t_curr = []
        self.t_post = []
        self.rs = []
        self.zeta = zeta
        self.r_g = r_g
        self.gamma_g = 0.9
        self.e_g = 0
        self.n_g = 0
        self.sum_Eg = []
    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha_tau = self.learning_rate
        gamma_tau = self.discount_rate

        # Q 値の更新を行う
        q_current_state, q_next_state, next_action = super().update(current_state, current_action, reward, next_state, next_action)

        # tau 値の更新
        t = self.t_curr + self.t_post
        self.t_curr[q_current_state, current_action] += 1
        self.t_post[q_current_state, current_action] += alpha_tau * (gamma_tau * t[q_next_state, next_action] - self.t_post[q_current_state, current_action])
        t = self.t_curr + self.t_post

        # 基準値 R の更新
        delta_g = min(self.e_g - self.r_g, 0)
        a = self.estimate_policy.select_action(self.Q, q_current_state)
        self.r[q_current_state] = self.Q[q_current_state][a] - self.zeta * delta_g

        # RS 値の更新
        self.rs = t * (self.Q.T - self.r).T

    def GRC_update(self, e_tmp):
        self.e_g = (e_tmp + self.gamma_g * (self.n_g * self.e_g)) / float(1 + self.gamma_g * self.n_g)
        # self.e_g = e_tmp
        self.n_g = 1.0 + self.gamma_g * self.n_g
        self.sum_Eg.append(self.e_g)

    def init_params(self):
        super().init_params()

        self.t_curr = np.zeros([self.num_state, self.num_action])
        self.t_post = np.zeros([self.num_state, self.num_action])
        self.rs = np.zeros([self.num_state, self.num_action])
        self.r = np.zeros(self.num_state)
        self.e_g = 0

    def get_value(self):
        return self.rs

"""# RS + GRC + 適格度トレース(off policy)"""

class RS_off_value_function_eligibility(Watkin_lambda):
    def __init__(self, num_state, num_action, learning_rate=0.1, discount_rate=0.9, r_g=0.75, zeta=0.05):

        super().__init__(num_state, num_action, learning_rate, discount_rate)

        self.r = []
        self.t_curr = []
        self.t_post = []
        self.rs = []
        self.zeta = zeta
        self.r_g = r_g
        self.gamma_g = 0.9
        self.e_g = 0
        self.n_g = 0
        self.sum_Eg = []

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha_tau = self.learning_rate
        gamma_tau = self.discount_rate

        # Q 値の更新を行う
        q_current_state, q_next_state, next_action = super().update(current_state, current_action, reward, next_state, next_action)

        # tau 値の更新
        t = self.t_curr + self.t_post
        self.t_curr[q_current_state, current_action] += 1
        self.t_post[q_current_state, current_action] += alpha_tau * (gamma_tau * t[q_next_state, next_action] - self.t_post[q_current_state, current_action])
        t = self.t_curr + self.t_post

        # 基準値 R の更新
        delta_g = min(self.e_g - self.r_g, 0)
        a = self.estimate_policy.select_action(self.Q, q_current_state)
        self.r[q_current_state] = self.Q[q_current_state][a] - self.zeta * delta_g

        # RS 値の更新
        self.rs = t * (self.Q.T - self.r).T

    def GRC_update(self, e_tmp):
        self.e_g = (e_tmp + self.gamma_g * (self.n_g * self.e_g)) / float(1 + self.gamma_g * self.n_g)
        # self.e_g = e_tmp
        self.n_g = 1.0 + self.gamma_g * self.n_g
        self.sum_Eg.append(self.e_g)

    def init_params(self):
        super().init_params()

        self.t_curr = np.zeros([self.num_state, self.num_action])
        self.t_post = np.zeros([self.num_state, self.num_action])
        self.rs = np.zeros([self.num_state, self.num_action])
        self.r = np.zeros(self.num_state)
        self.e_g = 0

    def get_value(self):
        return self.rs

"""# RS + GRC + 適格度トレース(on policy)"""

class RS_on_value_function_eligibility(Sarsa_lambda):
    def __init__(self, num_state, num_action, learning_rate=0.1, discount_rate=0.9, r_g=0.75, zeta=0.05):

        super().__init__(num_state, num_action, learning_rate, discount_rate)

        self.r = []
        self.t_curr = []
        self.t_post = []
        self.rs = []
        self.zeta = zeta
        self.r_g = r_g
        self.gamma_g = 0.9
        self.e_g = 0
        self.n_g = 0
        self.sum_Eg = []

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha_tau = self.learning_rate
        gamma_tau = self.discount_rate

        # Q 値の更新を行う
        q_current_state, q_next_state, next_action = super().update(current_state, current_action, reward, next_state, next_action)

        # tau 値の更新
        t = self.t_curr + self.t_post
        self.t_curr[q_current_state, current_action] += 1
        self.t_post[q_current_state, current_action] += alpha_tau * (gamma_tau * t[q_next_state, next_action] - self.t_post[q_current_state, current_action])
        t = self.t_curr + self.t_post
        # t = self.t_curr + gamma_tau * 0.9 * eligibility

        # 基準値 R の更新
        delta_g = min(self.e_g - self.r_g, 0)
        a = self.estimate_policy.select_action(self.Q, q_current_state)
        self.r[q_current_state] = self.Q[q_current_state][a] - self.zeta * delta_g

        # RS 値の更新
        self.rs = t * (self.Q.T - self.r).T

    def GRC_update(self, e_tmp):
        self.e_g = (e_tmp + self.gamma_g * (self.n_g * self.e_g)) / float(1 + self.gamma_g * self.n_g)
        # self.e_g = e_tmp
        self.n_g = 1.0 + self.gamma_g * self.n_g
        self.sum_Eg.append(self.e_g)

    def init_params(self):
        super().init_params()

        self.t_curr = np.zeros([self.num_state, self.num_action])
        self.t_post = np.zeros([self.num_state, self.num_action])
        self.rs = np.zeros([self.num_state, self.num_action])
        self.r = np.zeros(self.num_state)
        self.e_g = 0
        
    def get_value(self):
        return self.rs

class RS_tau(RS_off_value_function_eligibility):
    def __init__(self, num_state, num_action, learning_rate=0.1, discount_rate=0.9, r_g=0.75, zeta=0.05):

        super().__init__(num_state, num_action, learning_rate, discount_rate)

        self.r = []
        self.t_curr = []
        self.t_post = []
        self.rs = []
        self.zeta = zeta
        self.r_g = r_g
        self.gamma_g = 0.9
        self.e_g = 0
        self.n_g = 0
        self.sum_Eg = []

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha_tau = self.learning_rate
        gamma_tau = self.discount_rate

        # Q 値の更新を行う
        q_current_state, q_next_state, next_action, eligibility = super().update(current_state, current_action, reward, next_state, next_action)

        # tau 値の更新
        t = self.t_curr + self.t_post
        self.t_curr[q_current_state, current_action] += 1
        self.t_post[q_current_state, current_action] += alpha_tau * (gamma_tau * t[q_next_state, next_action] - self.t_post[q_current_state, current_action])
        t = self.t_curr + gamma_tau * 0.9 * eligibility

        # 基準値 R の更新

"""#ヒートマップ"""

class Heat_Map():
    def print_map(value_func, Q_value):
        Z = np.zeros(81)
        for i in range(81):
            Z[i] = (max(Q_value[i]))
        Z = np.array(Z).reshape(9,9)
        # print(Q_value)
        # print("Z")
        # print(Z)
        sns.heatmap(Z)

    def print_heat(tmp, w_x, w_y, all_action, log=False):
        if all_action:
            heat = np.zeros([3*w_x, 3*w_y])
            for x in range(w_x):
                for y in range(w_y):
                    state = x * w_y + y
                    outside = tmp[state].mean()
                    #outside = 0
                    center = tmp[state].max()
                    #center = 0
                    heat[3*x][3*y] = outside
                    heat[3*x][3*y+1] = tmp[state][0]
                    heat[3*x][3*y+2] = outside
                    heat[3*x+1][3*y] = tmp[state][3]
                    heat[3*x+1][3*y+1] = center
                    heat[3*x+1][3*y+2] = tmp[state][1]
                    heat[3*x+2][3*y] = outside
                    heat[3*x+2][3*y+1] = tmp[state][2]
                    heat[3*x+2][3*y+2] = outside
        else:
            heat = np.zeros([w_x, w_y])
            for x in range(w_x):
                for y in range(w_y):
                    state = x * w_y + y
                    heat[x][y] = max(tmp[state])
        fig = plt.subplots(figsize=(20, 6))
        if log == True:
            ##############################################
            data = heat + 1
            log_norm = LogNorm(vmin=data.min().min(), vmax=data.max().max())
            cbar_ticks = [math.pow(10, i) for i in range(math.floor(math.log10(data.min().min())), 1+math.ceil(math.log10(data.max().max())))]
            HEAT = sns.heatmap(
                data,
                norm=log_norm,
                cbar_kws={"ticks": cbar_ticks}, 
                cmap= "viridis", square=True
                # ,linewidths=.5
            )
            return HEAT
            ################################################
        else:
            HEAT = sns.heatmap(heat, annot=False, cbar=True, square=True, cmap= "viridis")
            return HEAT

"""#矢印マップ"""

class Arrow_Map():
    def print_map(value_func, Q_value):

        UP = 0
        DOWN = 1
        RIGHT = 3
        LEFT = 2

        for i in range(81):
            way = np.argmax(Q_value[i]) 
            if (max(Q_value[i])) == 0:
                print("*", end = " ")
            elif way == UP:
                print ("U", end = " ")
            elif way == DOWN:
                print ("D", end = " ")
            elif way == RIGHT:
                print ("R", end = " ")
            elif way == LEFT:
                print ("L", end = " ")
            
            if i % 9 == 8:
                print("\n")

    def print_q_direction(q, w_x, w_y):
        max_q_index = np.argmax(q, 1)
        for x in range(w_x):
            for y in range(w_y):
                state = y * w_y + x
                if all(max(q[state]) == q[state]):
                    print("_", end="")
                elif max_q_index[state] == 0:
                    print("^", end="")
                elif max_q_index[state] == 1:
                    print(">", end="")
                elif max_q_index[state] == 2:
                    print("v", end="")
                elif max_q_index[state] == 3:
                    print("<", end="")
                print("\t", end="")
            print()
        print()

"""#エージェント位置描画"""

class Grid_Map():
    def print_anim(width, height, _state, EPISODE):
        all_state = np.zeros((height, width))
        all_state[2][0] = 5
        all_state[6][0] = 5
        all_state[8][2] = 5
        all_state[2][8] = 5
        all_state[8][6] = 5
        all_state[6][8] = 5
        all_state[0][2] = 5
        all_state[0][6] = 5
        x = _state // height
        y = _state % height
        all_state[x][y] = 10
        plt.xlim(0, width)
        plt.ylim(height, 0)
        plt.xticks(range(width))
        plt.yticks(range(height))
        plt.title("EPISODE: " + str(EPISODE))
        rows = np.arange(width+ 1)
        cols = np.arange(height + 1)
        X, Y = np.meshgrid(cols, rows)
        plt.grid(True, which='both', axis='both', linestyle='-', color='black')
        im = plt.pcolor(X, Y, all_state, cmap=plt.cm.gray_r)
        
        # plt.show()
        # plt.close()
        return im

"""# エージェント"""

class Agent():
    def __init__(self, value_func="Q_Learning", policy="greedy", learning_rate=0.1, discount_rate=0.9, eps=None, eps_min=None, eps_decrease=None, n_state=None, n_action=None, lamda = 0.9, r_g = 7.5, zeta = 0.05):

        if value_func == "Q_Learning":
            self.value_func = Q_Learning(num_state=n_state, num_action=n_action)
        
        elif value_func == "Sarsa":
            self.value_func = Sarsa(num_state=n_state, num_action=n_action)

        elif value_func == "watkin":
            self.value_func = Watkin_lambda(num_state=n_state, num_action=n_action, lamda = 0.9)

        elif value_func == "Sarsa_lambda":
            self.value_func = Sarsa_lambda(num_state=n_state, num_action=n_action, lamda = 0.9)

        elif value_func == "RS":
            self.value_func = RS_value_function(num_state=n_state, num_action=n_action, r_g = r_g, zeta = zeta)
        
        elif value_func == "RS_onpolicy":
            self.value_func = RS_value_function_onpolicy(num_state=n_state, num_action=n_action, r_g = r_g, zeta = zeta)            

        elif value_func == "RS_on_eligibility":
            self.value_func = RS_on_value_function_eligibility(num_state=n_state, num_action=n_action, r_g = r_g, zeta = zeta)

        elif value_func == "RS_off_eligibility":
            self.value_func = RS_off_value_function_eligibility(num_state=n_state, num_action=n_action, r_g = r_g, zeta = zeta)

        else:
            print("error:価値関数候補が見つかりませんでした")
            sys.exit()

        if policy == "greedy":
            self.policy = Greedy()

        elif policy == "eps_greedy":
            self.policy = EpsGreedy(eps=eps)

        elif policy == "eps_dec_greedy":
            self.policy = EpsDecGreedy(eps=eps, eps_min=eps_min, eps_decrease=eps_decrease)

        else:
            print("error:方策候補が見つかりませんでした")
            sys.exit()

        self.current_state = self.next_state = 0
        self.n_state = n_state

    def update(self, current_action, reward, i):

        current_state = self.current_state
        next_state = self.next_state
     
        
        if i == "Sarsa" or i == "Sarsa lambda" or i == "Watkin lambda" or i == "RS on" or i == "RS(λ) on" or i == "RS(λ) off": 
            
            next_action = self.select_action_sarsa()
            self.value_func.update(current_state, current_action, reward, next_state, next_action)
        
        elif i == "Q Learning" or i == "RS off":
            
            self.value_func.update(current_state, current_action, reward, next_state)

        else:
            pass

        # self.policy.update_params()

    def GRC_update(self, e_tmp):

        self.value_func.GRC_update(e_tmp)

    def select_action(self):
        return self.policy.select_action(self.value_func.get_value(), self.current_state)
    
    def select_action_sarsa(self):
        return self.policy.select_action(self.value_func.get_value(), self.next_state)

    """
    def print_value(self):
        f = open('Q_value_waka.csv','w')
        writer = csv.writer(f, lineterminator = '\n')
        writer.writerows(self.value_func.get_value())
        print(self.value_func.get_value())
        f.close()
    """

    def print_value(self):
        self.value_func.print_value()

    def init_params(self):
        self.value_func.init_params()
        self.policy.init_params()
        self.current_state = self.next_state = 0

"""# メイン関数"""

goal1 = [2,6,26,62,78,74,54,18]
    goal2 = [26,62,78,74,54,18,2,6]
    goal3 = [78,74,54,18,2,6,26,62]
    goal4 = [54,18,2,6,26,62,78,74]
    # GOAL = []
    # GOAL.append(str(goal1))
    # GOAL.append(str(goal2))
    # GOAL.append(str(goal3))
    # GOAL.append(str(goal4))
    cliff = [27,28,29,30,32,48,50]
    # env = GlidWorld(5, 5, 0, 24)
    #env = Cliff_GlidWorld(7, 7, 42, 48, cliff)
    env = Cliff_optima_GlidWorld(9, 9, 40, goal1, cliff)


    SIMULATION_NUM = 100

    EPISODE_NUM = 500
    STEP_NUM = 100

    #アニメーション用配列
    ims = []    
    heat = []

    # GOAL = goal
    # RS(Rg、zeta)
    reward_RS = "RS off"
    reward_RS_onpolicy = "RS on"
    reward_RS_on_eligibility = "RS(λ) on"
    reward_RS_off_eligibility = "RS(λ) off"


    
    agents = {}
    
    # agents.update({"Q Learning": Agent(
    #     policy="eps_dec_greedy",
    #     eps=1.0,
    #     eps_min=0.0,
    #     eps_decrease=0.001,
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action()
    # )})
 
    # agents.update({"Sarsa": Agent(
    #     value_func = "Sarsa",
    #     policy="eps_dec_greedy",
    #     eps=1.0,
    #     eps_min=0.0,
    #     eps_decrease=0.001,
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action()
    # )})
    
    # agents.update({"Sarsa lambda": Agent(
    #     value_func="Sarsa_lambda",
    #     policy="eps_dec_greedy",
    #     eps=1.0,
    #     eps_min=0.0,
    #     eps_decrease=0.001,
    #     lamda = 0.9,
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action()
    # )})
    
    # agents.update({"Watkin lambda": Agent(
    #     value_func="watkin",
    #     policy="eps_dec_greedy",
    #     eps=1.0,
    #     lamda=0.9,
    #     eps_min=0.0,
    #     eps_decrease=0.001,
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action()
    # )})
    
    agents.update({reward_RS: Agent(
        value_func="RS",
        n_state=env.get_num_state(),
        n_action=env.get_num_action(),
        r_g = 7.5,
        zeta = 1
    )})

    agents.update({reward_RS_off_eligibility: Agent(
        value_func="RS_off_eligibility",
        n_state=env.get_num_state(),
        n_action=env.get_num_action(),
        r_g = 7.5,
        zeta = 1
    )})
    
    # agents.update({reward_RS_onpolicy: Agent(
    #     value_func="RS_onpolicy",
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action(),
    #     r_g = 7.5,
    #     zeta = 1
    # )})
    
    # agents.update({reward_RS_on_eligibility: Agent(
    #     value_func="RS_on_eligibility",
    #     n_state=env.get_num_state(),
    #     n_action=env.get_num_action(),
    #     r_g = 7.5,
    #     zeta = 0.5
    # )})
    
    reward_num = 8
    reward_prob = np.zeros(reward_num)
    every_reward = np.zeros(EPISODE_NUM) 
    reward_graph = np.zeros(EPISODE_NUM)
    step_graph = np.zeros(EPISODE_NUM)
    Eg_graph = np.zeros(EPISODE_NUM)
    greedy_Eg_graph = np.zeros(EPISODE_NUM)

    # reward_fig = plt.figure()
    print("start training")

    agents_reward_prob = {}
    agents_steps = {}
    agents_rewards = {}
    agents_Eg = {}
    agents_Greedy_Eg = {}
    Rg = np.full(EPISODE_NUM,7.5)
    fig = plt.figure()

    for i in agents.keys():
        print("{}'s training".format(i))
        step_graph = np.zeros(EPISODE_NUM)
        reward_graph = np.zeros(EPISODE_NUM)
        Eg_graph = np.zeros(EPISODE_NUM)
        greedy_Eg_graph = np.zeros(EPISODE_NUM)
        #学習開始
        for sim in range(SIMULATION_NUM):
            agents[i].init_params()
           
            every_reward = np.zeros(EPISODE_NUM)
            # reward_prob = np.zeros(reward_num)
            env.goal = goal1
            G = goal1
            for epi in range(EPISODE_NUM):
                # if epi > 2999:
                #     print("EPISODE: ", end = " ")
                #     print(epi)
                sum_reward = 0
                agents[i].current_state = env.get_start()
                ##############推定方策を確認するため##################################

                GreedyEg = 0
                Ng = 0
                for step in range(STEP_NUM):
                    action = TestGreedy.select_action(agents[i].value_func.Q, agents[i].current_state)
                    agents[i].next_state, reward = env.move(agents[i].current_state, action)
                    sum_reward += reward
                    agents[i].current_state = agents[i].next_state
                    if agents[i].current_state in G or step == STEP_NUM:
                        break
                
                GreedyEg = (sum_reward + 0.9 * (Ng * GreedyEg)) / float(1 + 0.9 * Ng)
                Ng = 1.0 + 0.9 * Ng

                greedy_Eg_graph[epi] += GreedyEg
                # ######################################################################
                #ステップ開始
                sum_reward = 0
                ##########報酬位置変更##################################################
                # if epi == 1000 :
                #     env.goal = goal2
                #     G = goal2
                # if epi == 2000:
                #     env.goal = goal3
                #     G = goal3
                # if epi == 3000:
                #     env.goal = goal4
                #     G = goal4
                # if epi == 4000:
                #     env.goal = goal1
                #     G = goal1
                # else:
                #     pass
                #########################################################################
                agents[i].current_state = env.get_start()
                ################学習開始#####################################################
                
                for step in range(STEP_NUM):
                    action = agents[i].select_action()
                    agents[i].next_state, reward = env.move(agents[i].current_state, action)
                    
                    sum_reward += reward
                    
                    agents[i].update(action, reward, i)

                    ##############エージェントの位置を描画できるやつ################
                    # print("epi", end = " ")
                    # print(epi)
                    # if epi == 60 :
                    #     im = Grid_Map.print_anim(9,9,agents[i].current_state, epi)
                    #     ims.append([im])
                    ################################################################
                    agents[i].current_state = agents[i].next_state

                    if agents[i].current_state in G or step == STEP_NUM:
                    # if agents[i].current_state == 24 or step == STEP_NUM:
        
                        # if epi > 2999:
                        # print("reward: ",end = " ")
                        # print(reward)
                        #     print("Eg: ",end = " ")
                        #     print(agents[i].value_func.e_g)
                        break
                agents[i].policy.update_params()
        
                #RSの時のみGRCの更新
                if  i == reward_RS or i == reward_RS_on_eligibility or i == reward_RS_off_eligibility  or i == reward_RS_onpolicy:
                    agents[i].GRC_update(sum_reward)
                    # agents[i].GRC_update(sum_reward / float(step + 1))
                else:
                    pass
                agents[i].value_func.Q.shape
                ################エピソード1での確認########################################################
                # if epi == 0:
                    # print(agents[i].value_func.Q)
                   
                ###############逐次的にヒートマップ出力########################################################
                if sim == 99:
                    # if epi == 999 or epi == 1999 or epi == 2999 or epi == 3999 or epi == 4999:
                    if epi == 99 or epi == 199:
                    # if epi < 8000 and epi > 5999:
                ######Q値ヒートマップ########
                        h = Heat_Map.print_heat(agents[i].value_func.Q, 9, 9, all_action=True, log=False)
                        #######RS値ヒートマップ#######
                        # h = Heat_Map.print_heat(agents[i].value_func.rs ,9,9 , all_action=True, log=False)
                        #######tau値ヒートマップ######
                        # h = Heat_Map.print_heat((agents[i].value_func.t_curr)+(agents[i].value_func.t_post) ,9,9 , all_action=True, log=False)
                        # heat.append([h])
                        plt.title("epi"+str(epi))
                        plt.savefig(str(i) + " Heatmap Qvalue EPISODE: " + str(epi) + "9.png")
                        plt.show()


                # print(agents[i].value_func.Q)
                # print(reward)
                #############################################################################################
                reward_prob[reward-1] += 1
                reward_graph[epi] += sum_reward
                # step_graph[epi] += step
                ###########毎ステップ収益グラフ##############
                every_reward[epi] += sum_reward
                ###########Egを表示グラフ######################
                Eg = agents[i].value_func.e_g
                Eg_graph[epi] += Eg

            #########毎エピソードのしゅつりょく#####################################################
            
            # plt.plot(every_reward, alpha = 0.5)
            # plt.tight_layout()
            # plt.title("every reward")
            # plt.show()

            ##########################################################################################################

        #############グラフのデータ保存###################################
        agents_Greedy_Eg.update({i: greedy_Eg_graph})
        agents_Eg.update({i:Eg_graph})
        agents_steps.update({i:step_graph})
        agents_rewards.update({i: reward_graph})
        agents_reward_prob.update({i: reward_prob})
        #########################################################################################

        # plt.tight_layout()
        # plt.title("every reward")
        # plt.savefig("Every reward.png")
        # plt.show()
        # print(i)
        # print_heat(agents[i].value_func.Q,9,9,all_action=True)
        #agents[i].print_value()
        # Arrow_Map.print_q_direction(agents[i].value_func.Q,9,9)
        # print_heat(agents[i].value_func.Q,9,9)
        # print_heat(agents[i].value_func.Q,9,9,all_action=True)
        # print("Tau: ",end = " ")
        # print((agents[i].value_func.t_curr)+(agents[i].value_func.t_post))

        print("finish")
    # print(env.goal)


    ###########選択確率########################################
    # print("選択確率")
    # print(reward_prob)

    # # グラフ作成/
    # reward_prob_fig = plt.figure()
    # cx = reward_prob_fig.add_subplot(111)
    # reward = np.array([1,2,3,4,5,6,7,8])
    # for i in agents.keys():
    #     # cx.bar(reward, agents_reward_prob[i]/ 100000)

    # cx.legend()  # 凡例を付ける
    # plt.ylim(4, 8)
    # cx.set_title("Reward Selection Probability")  # グラフタイトルを付ける
    # cx.set_ylim(0.0,1.0)
    # cx.set_xlabel("Reward")  # x軸のラベルを付ける
    # cx.set_ylabel("Probability")  # y軸のラベルを付ける
        
    # reward_prob_fig.tight_layout()
    # plt.savefig("Reward_Prob(aleph = 5.5).png")
    # plt.show()


    
    ###########アニメーショを作成###########################################

    # ani = animation.ArtistAnimation(fig, heat, blit=True, interval=1000, repeat_delay=1000)
    # ani.save("test.mp4", writer="ffmpeg")
    
    ###########結果を出力########################################

    reward_fig = plt.figure()
    
    ax = reward_fig.add_subplot(1,1,1)
    
    for i in agents.keys():
        ax.plot(agents_rewards[i] / SIMULATION_NUM, label=i)
        # ax.plot(agents_Eg[i] / SIMULATION_NUM, label=i)
        # ax.plot(agents_Greedy_Eg[i] / SIMULATION_NUM, label=str(i)+"Greedy")
    ax.plot(Rg, linestyle='--', color = '#000000')
    
    ax.legend()  # 凡例を付ける
    #plt.ylim(4, 8)
    ax.set_title("100 times average reward")  # グラフタイトルを付ける

    ax.set_xlabel("episode")  # x軸のラベルを付ける
    ax.set_ylabel("Eg")  # y軸のラベルを付ける
        
    reward_fig.tight_layout()
    plt.savefig("suboptima_RS,RS(λ).png")

    
    # Eg_fig = plt.figure()
    
    # bx = Eg_fig.add_subplot(111)
    # bx = reward_fig.add_subplot(2,1,2)

    # for i in agents.keys():
        # bx.plot(agents_Eg[i] / SIMULATION_NUM, label=i)
        # bx.plot(agents_Greedy_Eg[i] / SIMULATION_NUM, label="Q-Greedy", color = "#000000")
    # bx.plot(Rg, linestyle='--', color = '#000000')
    # bx.legend()  # 凡例を付ける
    #plt.ylim(4, 8)
    # bx.set_title("100 times average Eg")  # グラフタイトルを付ける
    # bx.set_xlabel("episode")  # x軸のラベルを付ける
    # bx.set_ylabel("Eg")  # y軸のラベルを付ける
    
    # Eg_fig.tight_layout()
    # reward_fig.suptitle("100 times average Eg")
    # plt.xlabel("episode")
    # plt.ylabel("Eg")

    # # reward_fig.tight_layout()
    # plt.savefig("RS(λ)_Eg.png")

    plt.show()  # グラフを表示
  
    
    # # step_fig = plt.figure()
    # bx = step_fig.add_subplot(111)
    # for i in agents.keys():
    #     bx.plot(agents_steps[i] / SIMULATION_NUM, label=i)

    # plt.legend()  # 凡例を付ける
    # plt.title("step")  # グラフタイトルを付ける
    # plt.xlabel("episode")  # x軸のラベルを付ける
    # plt.ylabel("sum step")  # y軸のラベルを付ける
    # plt.show()  # グラフを表示
    # plt.savefig("Step.png")

"""リスクを選んでいるのをわかっているのか？
多少確率をやっている人などの

*   エピソード75〜100で急に降下している
*   それまでは平均的に6をいた。
*   ヒートマップをみてみると、3ステップ以上離れている場所のQ値は大体同じになってしまっているから、そこまでつかないといけない
*   
*   リスト項目


*   6に立て続けて行動をしていて、収益が降下
*   ただ満足をしていないので、探索をする

#一番下
"""