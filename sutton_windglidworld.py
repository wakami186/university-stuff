# -*- coding: utf-8 -*-
"""WindGlidworld.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHqYRTZE8go59V9bnPntrxO1JTWeaTi1
"""

import numpy as np
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import seaborn as sns
import random
import sys
import csv

class Wind_GlidWorld():
    def __init__(self, height, length, start, goal, wind):
        self.reward = -1
        self.num_state = height * length
        self.num_action = 4
        self.height = height
        self.length = length
        self.start = start
        self.goal = goal
        self.wind = wind

    def to_s(self, row, col):
        return ((col * self.length) + row)

    def to_p(self, s):
        y = s // self.length
        x = s % self.length
        return x, y

    def move(self, state, action):
        UP = 0
        RIGHT = 1
        DOWN = 2
        LEFT = 3

        column, row = self.to_p(state)
        tmp_col = column
        tmp_row = row
        # print("state: {},".format(state),end=" ")
        # print("x: {}".format(column),end=" ")
        # print("y: {}".format(row),end=" ")
        if action == UP:
            if (row) > 0:
                row -= 1
        elif action == DOWN:
            if (row) < (self.height - 1):
                row += 1
        elif action == LEFT:
            if (column) > 0:
                column -= 1
        elif action == RIGHT:
            if (column) < (self.length - 1):
                column += 1

        if (row - self.wind[tmp_col]) < 0:
            row = 0
        else:
            row = row - self.wind[tmp_col]

        


        new_state = self.to_s(column, row)

        reward = self.get_reward(new_state)
        # print("new_state: {},".format(new_state),end=" ")
        # print("x: {}".format(column),end=" ")
        # print("y: {}".format(row))
        return new_state, reward

    def get_reward(self, state):

        return -1

    def get_current_state(self):
        return self.current_state

    def get_next_state(self):
        return self.next_state
        
    def get_num_state(self):
        return self.num_state

    def get_num_action(self):
        return self.num_action
    
    def get_start(self):
        return self.start

    def get_goal_state(self):
        return self.goal

class Greedy(object):
    def select_action(self, value, state):

        idx = np.where(value[state] == max(value[state]))
        return random.choice(idx[0])

    def init_params(self):
        pass

    def update_params(self):
        pass

class EpsGreedy(Greedy):
    def __init__(self, eps = 0.1):
        self.eps = eps

    def select_action(self, value, state):
        if random.random() < self.eps:
            return random.choice(range(len(value[state])))
        else:
            return super().select_action(value, state)

    def init_params(self):
        pass

    def update_params(self):
        pass

class Sarsa(object):
    def __init__(self, num_state, num_action, learning_rate = 0.1, discount_rate = 0.9):
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.num_state = num_state
        self.num_action = num_action

        self.Q = np.zeros((self.num_state, self.num_action))

    def update(self, current_state, current_action, reward, next_state, next_action):
        alpha = self.learning_rate
        gamma = self.discount_rate

        TD_error = reward + gamma * self.Q[next_state, next_action] - self.Q[current_state, current_action]
        self.Q[current_state, current_action] += alpha * TD_error

        return current_state, next_state, next_action

    def init_params(self):
        self.Q = np.zeros((self.num_state, self.num_action))
    
    def get_value(self):
        return self.Q

class Agent():
    def __init__(self, value_func = "Sarsa", policy = "epsgreedy", learning_rate = 0.5, discount_rate = 1, epsilon = None, nstate = None, naction = None):
        # learning_rate 学習率α , discount_rate 割引率γ , nstate 総状態数 , naction 総行動数
        if value_func == "Sarsa":
            self.value_func = Sarsa(num_state = nstate, num_action = naction)

        else:
            print("error:価値関数候補が見つかりませんでした")
            sys.exit()

        if policy == "greedy":
            self.policy = Greedy()

        elif policy == "eps_greedy":
            self.policy = EpsGreedy(eps=epsilon)

        else:
            print("error:方策候補が見つかりませんでした")
            sys.exit()


    def update(self, current_action, reward, i):

        current_state = self.current_state
        next_state = self.next_state
     
        
        if i == "Sarsa" : 
            next_action = self.select_action_sarsa()
            self.value_func.update(current_state, current_action, reward, next_state, next_action)
    
        else:
            pass



    def select_action(self):
        return self.policy.select_action(self.value_func.get_value(), self.current_state)
    
    def select_action_sarsa(self):
        return self.policy.select_action(self.value_func.get_value(), self.next_state)

    def print_value(self):
        self.value_func.print_value()

    def init_params(self):
        self.value_func.init_params()
        self.policy.init_params()
        self.current_state = self.next_state = 0

class Arrow_Map():
    def print_map(Q_value):

        UP = 0
        RIGHT = 1
        DOWN = 2
        LEFT = 3

        for i in range(70):
            way = np.argmax(Q_value[i]) 
            if (max(Q_value[i])) == 0:
                print("*", end = " ")
            elif way == UP:
                print ("↑", end = " ")
            elif way == DOWN:
                print ("↓", end = " ")
            elif way == RIGHT:
                print ("→", end = " ")
            elif way == LEFT:
                print ("←", end = " ")
            
            if i % 10 == 9:
                print("\n")

def main():
  
    # 風
    wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0] 
    # 環境設定
    env = Wind_GlidWorld(7, 10, 30, 37, wind)

    SIMULATION_NUM = 1
    EPISODE_NUM = 170
    STEP_NUM = 1000

    reward_graph = np.zeros(EPISODE_NUM)
    episode_graph = np.zeros(EPISODE_NUM)
    step_graph = np.zeros(EPISODE_NUM)
    agents_rewards = {}
    sum_step = 0
    agents = {}

    agents.update({"Sarsa": Agent(
        value_func = "Sarsa",
        policy="eps_greedy",
        epsilon=0.1,
        nstate=env.get_num_state(),
        naction=env.get_num_action()
    )})

    for i in agents.keys():
        print ("{} train". format(i))
        reward_graph = np.zeros(EPISODE_NUM)
        for sim in range(SIMULATION_NUM):
            agents[i].init_params()
            for epi in range(EPISODE_NUM):
                episode_graph[epi] = epi
                # print("epi: {}".format(epi))
                sum_reward = 0
                agents[i].current_state = env.get_start()
                for step in range(STEP_NUM):
                    
                    action = agents[i].select_action()
                    # print("step: {} ,".format(step), end = " ")
                    # print("current state: {},".format(agents[i].current_state),end = " ")
                    # print("action: {}".format(action), end = " ")

                    agents[i].next_state, reward = env.move(agents[i].current_state, action)
                    
                    sum_reward += reward
                    
                    agents[i].update(action, reward, i)
                    
                    agents[i].current_state = agents[i].next_state

                    if agents[i].current_state == 37 or step == STEP_NUM:
                        sum_step += step
                        break
                reward_graph[epi] += sum_reward
                step_graph[epi] += sum_step
                

        agents_rewards.update({i: reward_graph})
        Arrow_Map.print_map(agents[i].value_func.get_value())
    # print(agents_rewards[i]/100)

    plt.figure()
    # for i in agents.keys():
        # plt.plot(agents_rewards[i] / SIMULATION_NUM, label = i)
    plt.plot(step_graph, episode_graph)
    
    plt.legend()
    plt.title("wind")
    plt.xlabel("episode")
    plt.ylabel("step")
    plt.show()
                # print ("Reward: {}". format(sum_reward))

main()